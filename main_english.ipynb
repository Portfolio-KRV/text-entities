{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Recurrent neural networks to recognize entities in texts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Developed by Kevin Reyes - Diego Quezada.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
        "np.random.seed(1)\n",
        "random.seed(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "session_conf = tf.compat.v1.ConfigProto(\n",
        "    intra_op_parallelism_threads=1, inter_op_parallelism_threads=1\n",
        ")\n",
        "tf.compat.v1.set_random_seed(1)\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "tf.compat.v1.keras.backend.set_session(sess)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# NN over text\n",
        "\n",
        " Today, a relevant application of recurrent neural networks is text and natural language modeling. In this work we will address the problem of processing text sentences, provided by GMB ( *Groningen Meaning Bank* ), for entity recognition and tagger. Specifically, we will work with the dataset provided through **[Kaggle](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)** , which is made up of more than a million words, in order to make predictions on different tasks such as *many to many* and *many to one* .\n",
        "\n",
        " <img src=\"https://i.stack.imgur.com/b4sus.jpg\" width=\"70%\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## a) Data loading and pre-processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Dataset Comments:**\n",
        "\n",
        " The original task of the dataset was to identify named (or tagged) features from text, particularly features such as name and location.\n",
        " The variable we seek to predict is the entity or entities that participate in a sentence. this refers to identifying crucial information or of interest\n",
        " of a text. A dictionary would not suffice because it would give us limited information that would not be considered a recurrent architecture. The latter allows us to identify entities of interest in\n",
        " complex sentences where the entity is not explicit, so it is not possible to obtain a correct prediction from a simple dictionary.\n",
        " In this first instance we will work with the task of performing a NER *tag* ( **Named Entity Recognition** ) on each of the words in the sentences that are presented to us in the data. This task is of type *many to many* , that is, the input is a sequence and the output is a sequence, without *shift* , so we will need a suitable network structure for this. First we will extract the columns that we will use from the dataset.\n",
        "\n",
        " **Comments:**\n",
        "\n",
        " We will use the lemmatization of each word since each word can have multiple variations, for example, *working* in English in the different\n",
        " times and people could be: work, worked, working and works, each of these words has\n",
        " the same meaning but in a different time or person, so it is useful to use\n",
        " lemmatization to bring all the words to their root and thus get the most out of it\n",
        " to learning, since, instead of having to learn work, worked, working and works, we will be\n",
        " learning work and its use can be interpreted given the context, this is useful for our\n",
        " specific problem, since, to recognize entities in a sequence it is not relevant\n",
        " the time or the person used.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "entity-annotated-corpus.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "entity-annotated-corpus.zip  ner.csv  ner_dataset.csv\n"
          ]
        }
      ],
      "source": [
        "username=\"diegoquezada21\"\n",
        "key=\"afb1882970f4ac5726f12dda7bf92f7c\"\n",
        "!pip install -q kaggle\n",
        "api_token = {\"username\":username,\"key\":key}\n",
        "import json\n",
        "import zipfile\n",
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = str(username)\n",
        "os.environ['KAGGLE_KEY'] = str(key)\n",
        "!kaggle datasets download -d abhinavwalia95/entity-annotated-corpus\n",
        "if not os.path.exists(\"/content/NER\"):\n",
        "    os.makedirs(\"/content/NER\")\n",
        "os.chdir('/content/NER')\n",
        "for file in os.listdir():\n",
        "    if file[-4:]==\".zip\":\n",
        "      zip_ref = zipfile.ZipFile(file, 'r')\n",
        "      zip_ref.extractall()\n",
        "      zip_ref.close()\n",
        "!ls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lemma</th>\n",
              "      <th>tag</th>\n",
              "      <th>word</th>\n",
              "      <th>sentence_idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>thousand</td>\n",
              "      <td>O</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>of</td>\n",
              "      <td>O</td>\n",
              "      <td>of</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>demonstr</td>\n",
              "      <td>O</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>have</td>\n",
              "      <td>O</td>\n",
              "      <td>have</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>march</td>\n",
              "      <td>O</td>\n",
              "      <td>marched</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>.</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>iranian</td>\n",
              "      <td>B-gpe</td>\n",
              "      <td>Iranian</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>offici</td>\n",
              "      <td>O</td>\n",
              "      <td>officials</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>say</td>\n",
              "      <td>O</td>\n",
              "      <td>say</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>they</td>\n",
              "      <td>O</td>\n",
              "      <td>they</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        lemma    tag           word  sentence_idx\n",
              "0    thousand      O      Thousands           1.0\n",
              "1          of      O             of           1.0\n",
              "2    demonstr      O  demonstrators           1.0\n",
              "3        have      O           have           1.0\n",
              "4       march      O        marched           1.0\n",
              "..        ...    ...            ...           ...\n",
              "195         .      O              .           9.0\n",
              "196   iranian  B-gpe        Iranian          10.0\n",
              "197    offici      O      officials          10.0\n",
              "198       say      O            say          10.0\n",
              "199      they      O           they          10.0\n",
              "\n",
              "[200 rows x 4 columns]"
            ]
          },
          "execution_count": 59,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df_ner = pd.read_csv(\"ner.csv\", encoding=\"cp1252\", error_bad_lines=False)\n",
        "df_ner.dropna(inplace=True)\n",
        "dataset = df_ner.loc[:, [\"lemma\", \"tag\", \"word\", \"sentence_idx\"]]\n",
        "dataset[0:200]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "##### II) In order to use this data set, we must transform our table of words and sentences, to a table where each entry is a sentence, also encoding the different lemmas and tags as numerical values. This could be done with one of the keras or sklearn utilities, however the following code proposes a method only using python and pandas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Crea un diccionario donde a cada lemma se le asigna un código numérico partiendo desde el 1\n",
        "lemma_to_code = {lemma: code + 1 for code, lemma in enumerate(dataset.lemma.unique())}\n",
        "# Crea un diccionario donde a cada tag se le asigna un código numérico partiendo desde el 1\n",
        "tag_to_code = {tag: code + 1 for code, tag in enumerate(dataset.tag.unique())}\n",
        "# Calcula la cantidad de lemmas\n",
        "n_lemmas = len(lemma_to_code)\n",
        "lemmas = dataset.lemma.unique()\n",
        "tags = dataset.tag.unique()\n",
        "# cambia el valor de cada registro de lemma en el dataset por el código numérico generado\n",
        "dataset[\"lemma\"] = dataset.lemma.apply(lambda x: lemma_to_code[x])\n",
        "# cambia el valor de cada registro de tag en el dataset por el código numérico generado\n",
        "dataset[\"tag\"] = dataset.tag.apply(lambda x: tag_to_code[x])\n",
        "\n",
        "# Primero las palabras se agrupan por la sentencia a la que pertenecen\n",
        "# Cada grupo se transforma en una lista y luego se convierte en un array de numpy\n",
        "dff = dataset.groupby(\"sentence_idx\")[[\"lemma\", \"tag\"]].agg(list).applymap(np.asarray)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## b) Distributions.\n",
        "\n",
        " Now that we have the sentences encoded and grouped, we will explore the size of these, in number of lemmas.\n",
        "\n",
        " **Answer:** Not all sentences are the same size, this makes sense because in practice you will not work with sentences of the same length.\n",
        " When training RNN&#39;s it is necessary to define the form of the input, however these do not depend on the length of the sequence when inferring.\n",
        " The problems that this could cause occur in training, when there is not a perfect match between the size of each input with the total length.\n",
        " of the sequence so that at some point the network must be fed with an input of different dimensions, in these cases it can be **truncated** or used **padding** .\n",
        "\n",
        " It is highlighted that the classes are not distributed equally, sentences with a length of approximately 20 words are overrepresented.\n",
        "\n",
        " **Zipf Law (law on the distribution of words):** Indeed we can observe the Zipf law. This is evidenced in our last graph in this section, where we see that\n",
        " By plotting the lemmas according to their frequency from highest to lowest, we obtain a function with the same behavior as one of the form $\\displaystyle f(x) = \\frac{1}{x^a}$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20243\n",
            "17\n"
          ]
        }
      ],
      "source": [
        "print(n_lemmas)\n",
        "n_tags = len(tag_to_code)\n",
        "print(n_tags)\n",
        "# print(dff['lemma'].applymap(len))\n",
        "largos_sentencias = list(map(len, dff[\"lemma\"].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.hist(largos_sentencias, bins=100)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f40319fa128>"
            ]
          },
          "execution_count": 63,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "lemma_frequence_2 = (\n",
        "    pd.Series(np.concatenate([x for x in dff[\"lemma\"]])).value_counts().sort_index()\n",
        ")\n",
        "# plt.hist(lemma_frequence_2.values,bins = lemma_frequence_2.index.map(str))\n",
        "# plt.hist(lemma_frequence_2.values, bins = lemma_frequence_2.index.to_numpy())\n",
        "lemma_frequence_2.plot(figsize=(10, 10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## c) Padding and one hot vectors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "##### I) Next, we will transform the lemma sequences (and the corresponding tags) so that they all have the same length, that is, we will apply padding. The padding will be done with the value 0.\n",
        "\n",
        " **Comments:**\n",
        "\n",
        " In English, Spanish and other languages, grammatically the entities\n",
        " are usually named at the beginning of sentences, for example, \"The organization achieved its\n",
        " objectives this year\". Therefore, we should ensure that the neurons that receive\n",
        " the initial part of the sentences work with valid data, for this we will apply the padding at the end of the sentence.\n",
        "\n",
        " The padding value, in this case zero, must be chosen depending on the dictionary, in our case the numerical encoding\n",
        " of the lemmas starts from 1, so when padding we must avoid\n",
        " use a number that is part of the lemmas, since, in that case, we would be adding\n",
        " words with meaning to the sentence, when using the 0 that is not part of the dictionary\n",
        " we are adding a neutral element.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29.871620661227507\n"
          ]
        }
      ],
      "source": [
        "mean_sentencias = sum(largos_sentencias) / len(largos_sentencias)\n",
        "print(mean_sentencias)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_len = 30  # Promedio de largos de secuencias\n",
        "X = pad_sequences(\n",
        "    dff[\"lemma\"], maxlen=max_len, padding=\"post\"\n",
        ")  # 0 al inicio por default\n",
        "Y = dff[\"tag\"].values\n",
        "Y = pad_sequences(Y, maxlen=max_len, padding=\"post\")  # 0 al inicio por default\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "##### II) In order to provide a classification of the different *tags* , it is necessary to transform them into *one hot vectors* , since they are encoded in integers, this will result in a three-dimensional array with the number of examples, the maximum number of words and the number of possible *tags* .\n",
        "\n",
        " **Comments:**\n",
        "\n",
        " **Input dimensions:** For the training set we have 26382 examples and to validate we have 8795 , where each one is an array of 30 lemmas, that is,\n",
        " a numerically encoded statement.\n",
        "\n",
        " **Output dimensions:** For the training set we have 26382 examples and to validate we have 8795 , where each output element is an array with the\n",
        " complete statement, which in turn each element of the statement has an associated array of dimension 18 (it should be 17, but due to a keras error, 1 was added to the length of the one-hot-vector),\n",
        " which has in the element $i$ the probability that this lemma is of the class/tag $i$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "y = np.asarray([to_categorical(i, num_classes=n_tags + 1) for i in Y])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_tr, x_val, y_tr, y_val = train_test_split(X, y, test_size=0.25, random_state=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((26382, 30), (26382, 30, 18), (8795, 30), (8795, 30, 18))"
            ]
          },
          "execution_count": 68,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_tr.shape, y_tr.shape, x_val.shape, y_val.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## d) RNN many to many\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "##### I) We will use a *many to many* recurrent neural network with LSTM gates to learn how to *tag* the entity in the text. This network must process the sequence of filled (or unfilled) *lemmas* and deliver the *tag* to each of the *lemmas* , so the output of the network is not a vector as previously worked, but has an extra dimension: which is because at each instant of time an *output* needs to be delivered. As the *lemmas* correspond to essentially categorical, or at least discrete, data, it is necessary to generate a vector representation of them. The first layer of the network to be built must therefore include a trainable transformation from the original (discrete) representation space to ${\\rm I!R}^{d}$ , with $d$ the dimensionality of the *embedding* .\n",
        "\n",
        " **Comments:**\n",
        "1.  Input -&gt; [30].\n",
        "1.  Embedding -&gt; [30, 32]\n",
        "1.  LSTM -&gt; [30, 128]\n",
        "1.  Give yourself -&gt; [30,18]\n",
        "\n",
        " When inputting a sequence to embedding, each lemma in the sequence is converted to a 32-dimensional vector representation, so the output is a sequence of length 30\n",
        " where each element is a vector of 32. This sequence is fed to the LSTM, which returns a sequence of 30 elements (return_sequences=True), where each\n",
        " element has the output of all 128 cells of the LSTM. Finally this feeds into the dense layer, which returns the sequence of 30 elements, where each element\n",
        " is a vector of length 18 (17+1), where the element $i$ of the vector represents the probability of the lemma being of the tag $i$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Embedding, Dense\n",
        "\n",
        "x_tr = np.asarray(x_tr).astype(\"float32\")\n",
        "x_val = np.asarray(x_val).astype(\"float32\")\n",
        "max_len = 30\n",
        "\n",
        "# ¿problemas con el embedding al ejecutar? chequear que el n_lemas, n_tags, y max_len correspondan a los datos modificados con padding\n",
        "m = Sequential()\n",
        "embedding_dim = 32\n",
        "m.add(Embedding(input_dim=n_lemmas + 1, output_dim=embedding_dim, input_length=max_len))\n",
        "m.add(LSTM(units=128, return_sequences=True))\n",
        "m.add(Dense(n_tags + 1, activation=\"softmax\"))\n",
        "m.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n",
        "# history = m.fit(x_tr,y_tr, validation_data=(x_val,y_val), epochs=3, batch_size=128)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 30, 32)            647808    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 30, 128)           82432     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 30, 18)            2322      \n",
            "=================================================================\n",
            "Total params: 732,562\n",
            "Trainable params: 732,562\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "m.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install sklearn-crfsuite\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "To evaluate the model we will use a suitable metric for the imbalance present between the classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.95      0.91    152888\n",
            "           1       0.98      0.91      0.94    206567\n",
            "           2       0.84      0.49      0.62      8734\n",
            "           3       0.99      0.09      0.16      3771\n",
            "           4       0.82      0.19      0.30      3633\n",
            "           5       0.71      0.12      0.21      1700\n",
            "           6       0.00      0.00      0.00      4577\n",
            "           7       0.00      0.00      0.00      3712\n",
            "           8       0.95      0.49      0.65      4643\n",
            "           9       0.00      0.00      0.00        83\n",
            "          10       0.00      0.00      0.00        55\n",
            "          11       0.63      0.56      0.60      3717\n",
            "          12       0.00      0.00      0.00        43\n",
            "          13       1.00      0.03      0.06      1409\n",
            "          14       0.00      0.00      0.00        60\n",
            "          15       0.00      0.00      0.00        84\n",
            "          16       0.00      0.00      0.00        76\n",
            "          17       0.00      0.00      0.00        23\n",
            "\n",
            "   micro avg       0.92      0.87      0.89    395775\n",
            "   macro avg       0.43      0.21      0.25    395775\n",
            "weighted avg       0.91      0.87      0.87    395775\n",
            " samples avg       0.87      0.87      0.87    395775\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn_crfsuite.metrics import flat_classification_report\n",
        "\n",
        "\n",
        "y_pred = m.predict(x_val)\n",
        "y_pred = y_pred > 0.5\n",
        "\n",
        "\n",
        "print(flat_classification_report(y_val, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "##### II) We will modify the dimensionality of the initial embedding and determine if it **increases or decreases the classification error**\n",
        "\n",
        " **Comments:**\n",
        "\n",
        " When experimenting with dimensionalities greater than 30 for the embedding we do not obtain a considerable improvement based on the\n",
        " precision by weighted average. With a dimensionality of 25 we obtain results practically the same as those obtained with 30, however\n",
        " by starting to further decrease the dimensionality of the embeddng we get worse results. This is clear then\n",
        " we are excessively reducing the dimensionality of the mathematical representation of our vocabulary (lemmas), so the embedding layer fails to learn\n",
        " all the patterns between the lemmas of the dataset, that is, from a certain dimension downwards, a problem of underfitting of the vocabulary used appears.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "207/207 [==============================] - 5s 16ms/step - loss: 1.4532 - acc: 0.7412 - val_loss: 0.4648 - val_acc: 0.8822\n",
            "Epoch 2/3\n",
            "207/207 [==============================] - 3s 13ms/step - loss: 0.3986 - acc: 0.8913 - val_loss: 0.2957 - val_acc: 0.9137\n",
            "Epoch 3/3\n",
            "207/207 [==============================] - 3s 14ms/step - loss: 0.2712 - acc: 0.9232 - val_loss: 0.2270 - val_acc: 0.9433\n"
          ]
        }
      ],
      "source": [
        "# ¿problemas con el embedding al ejecutar? chequear que el n_lemas, n_tags, y max_len correspondan a los datos modificados con padding\n",
        "m = Sequential()\n",
        "embedding_dim = 25\n",
        "m.add(Embedding(input_dim=n_lemmas + 1, output_dim=embedding_dim, input_length=max_len))\n",
        "m.add(LSTM(units=128, return_sequences=True))\n",
        "m.add(Dense(n_tags + 1, activation=\"softmax\"))\n",
        "m.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n",
        "history = m.fit(x_tr, y_tr, validation_data=(x_val, y_val), epochs=3, batch_size=128)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     54637\n",
            "           1       0.97      0.98      0.98    178340\n",
            "           2       0.86      0.52      0.65      7537\n",
            "           3       0.99      0.12      0.21      3143\n",
            "           4       0.82      0.22      0.34      3033\n",
            "           5       0.72      0.13      0.22      1482\n",
            "           6       0.00      0.00      0.00      3821\n",
            "           7       0.00      0.00      0.00      3139\n",
            "           8       0.96      0.58      0.72      4017\n",
            "           9       0.00      0.00      0.00        68\n",
            "          10       0.00      0.00      0.00        44\n",
            "          11       0.82      0.57      0.67      3121\n",
            "          12       0.00      0.00      0.00        33\n",
            "          13       1.00      0.03      0.06      1228\n",
            "          14       0.00      0.00      0.00        50\n",
            "          15       0.00      0.00      0.00        72\n",
            "          16       0.00      0.00      0.00        64\n",
            "          17       0.00      0.00      0.00        21\n",
            "\n",
            "   micro avg       0.97      0.91      0.94    263850\n",
            "   macro avg       0.45      0.23      0.27    263850\n",
            "weighted avg       0.94      0.91      0.91    263850\n",
            " samples avg       0.91      0.91      0.91    263850\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "y_pred = m.predict(x_val)\n",
        "y_pred = y_pred > 0.5\n",
        "print(flat_classification_report(y_val, y_pred))  # , labels=tag_to_code))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## e) Bidirectional RNN and masking\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "##### I) Some authors point out the important dependency that exists in the text, not only with the previous words, but also with the following ones. **We will improve the network defined in 2.dI) using a Bidirectional recurrent neural network** , that is, with recurrence in both directions on the input *lemma* sequence.\n",
        "\n",
        " We get better results using the merge_mode parameter in sum. This is because, as stated above, the most relevant information is obtained by moving from the beginning to the end (forward), so that if, for example, we use mult and the past indicates a probability close to 1, while the future (backward) a close to zero when doing merge we will get an output close to zero. When using sum we simply take the probability that the past tells us (forward) for cases where the future tells us a probability close to 0. Note also that using concat does not discard information because we do not do a merge as such, in fact , increases the dimensionality of the LSTM output and the task of identifying the importance weightings falls on the dense layer, however, for this application this results in an unnecessary expenditure of computational resources.\n",
        "\n",
        " Using this type of network slightly decreased performance. In problems like predicting the next word in a sentence, it is really useful to know the future context (what is expected to be said after the word to be predicted), whereas in the entity type prediction problem, given the way that we structure language normally words\n",
        " that we used before allow us to more easily recognize an entity that will be named. In example 1, we note how before mentioning **Hyde Park**\n",
        " the word **in** is used, which is a recurring pattern when talking about a physical place with certain characteristics (in English), the words that\n",
        " appear after **Hyde Park** , they are not really useful, since they are very specific, so they will not help to improve the generalization capacity of the model.\n",
        " In example 2, we see the same for **Britain&#39;s** , the word **of** is a typical pattern before mentioning a B-Geo, the same rule can be applied for\n",
        " the rest of the words in bold, usually the most important thing to recognize the type of an entity is in the past context of the sentence.\n",
        "\n",
        " Examples:\n",
        "1.  They marched from the houses of parliament to a rally in **Hyde Park** . Police put the number of marchers at 10,000...\n",
        "1.  the annual conference of **Britain&#39;s** ruling **Labor Party** in the **southern English** seaside resort of Brighton. The party is...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 30, 25)            506100    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 30, 256)           157696    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 30, 18)            4626      \n",
            "=================================================================\n",
            "Total params: 668,422\n",
            "Trainable params: 668,422\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "207/207 [==============================] - 7s 21ms/step - loss: 1.2446 - acc: 0.7559 - val_loss: 0.3945 - val_acc: 0.8862\n",
            "Epoch 2/3\n",
            "207/207 [==============================] - 3s 16ms/step - loss: 0.3403 - acc: 0.9000 - val_loss: 0.2329 - val_acc: 0.9350\n",
            "Epoch 3/3\n",
            "207/207 [==============================] - 3s 16ms/step - loss: 0.1984 - acc: 0.9452 - val_loss: 0.1616 - val_acc: 0.9548\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(\n",
        "    Embedding(input_dim=n_lemmas + 1, output_dim=embedding_dim, input_length=max_len)\n",
        ")\n",
        "layer_lstm = LSTM(units=128, return_sequences=True)\n",
        "model.add(\n",
        "    Bidirectional(layer_lstm, merge_mode=\"concat\")\n",
        ")  # 'sum', 'mul', 'concat', 'ave'\n",
        "model.add(Dense(n_tags + 1, activation=\"softmax\"))\n",
        "model.summary()\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n",
        "history = model.fit(\n",
        "    x_tr, y_tr, validation_data=(x_val, y_val), epochs=3, batch_size=128\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     54637\n",
            "           1       0.98      0.98      0.98    178340\n",
            "           2       0.86      0.69      0.76      7537\n",
            "           3       0.95      0.87      0.91      3143\n",
            "           4       0.83      0.54      0.66      3033\n",
            "           5       0.79      0.45      0.58      1482\n",
            "           6       0.90      0.18      0.30      3821\n",
            "           7       0.83      0.13      0.23      3139\n",
            "           8       0.92      0.72      0.81      4017\n",
            "           9       0.00      0.00      0.00        68\n",
            "          10       0.00      0.00      0.00        44\n",
            "          11       0.81      0.69      0.75      3121\n",
            "          12       0.00      0.00      0.00        33\n",
            "          13       0.81      0.37      0.50      1228\n",
            "          14       0.00      0.00      0.00        50\n",
            "          15       0.00      0.00      0.00        72\n",
            "          16       0.00      0.00      0.00        64\n",
            "          17       0.00      0.00      0.00        21\n",
            "\n",
            "   micro avg       0.97      0.94      0.96    263850\n",
            "   macro avg       0.54      0.37      0.42    263850\n",
            "weighted avg       0.97      0.94      0.94    263850\n",
            " samples avg       0.94      0.94      0.94    263850\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "y_pred_2e = model.predict(x_val)\n",
        "y_pred_2e = y_pred_2e > 0.5\n",
        "print(flat_classification_report(y_val, y_pred_2e))  # , labels=tag_to_code))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "##### II) The *Masking* layer has recently been implemented in recurrent networks in *keras* , which could bring great help thanks to the *padding* that is done with the special symbol defined. We will train the network defined in 2.dI).\n",
        "\n",
        " **Comments:**\n",
        "\n",
        " By using mask_zero = True we get worse performance, apparently it doesn&#39;t work for us because we applied it wrongly. However, we can also mask the value 0 by adding a Masking layer after the Embedding, for this case we did get better results.\n",
        "\n",
        " Using Masking we obtain for the weighted average the values 0.97, 0.96 and 0.96. For the model of question 2.dI we obtain the values 0.91, 0.87 and 0.87.\n",
        "\n",
        " This improvement is to be expected because, unlike the previous model, now we are not taking into account the zeros introduced by padding in learning, that is, we are reducing a source of error in the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "207/207 [==============================] - 13s 41ms/step - loss: 1.1027 - acc: 0.8324 - val_loss: 0.2714 - val_acc: 0.9259\n",
            "Epoch 2/3\n",
            "207/207 [==============================] - 7s 35ms/step - loss: 0.2342 - acc: 0.9354 - val_loss: 0.1548 - val_acc: 0.9591\n",
            "Epoch 3/3\n",
            "207/207 [==============================] - 7s 35ms/step - loss: 0.1330 - acc: 0.9644 - val_loss: 0.1176 - val_acc: 0.9669\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import Masking\n",
        "\n",
        "m_2e2 = Sequential()\n",
        "m_2e2.add(Embedding(input_dim=n_lemmas + 1, output_dim=embedding_dim))\n",
        "m_2e2.add(Masking(mask_value=0.0))\n",
        "m_2e2.add(LSTM(units=128, return_sequences=True))\n",
        "m_2e2.add(Dense(n_tags + 1, activation=\"softmax\"))\n",
        "m_2e2.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n",
        "history = m_2e2.fit(\n",
        "    x_tr, y_tr, validation_data=(x_val, y_val), epochs=3, batch_size=128\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    152888\n",
            "           1       0.97      0.99      0.98    206567\n",
            "           2       0.85      0.72      0.78      8734\n",
            "           3       0.96      0.87      0.91      3771\n",
            "           4       0.83      0.66      0.73      3633\n",
            "           5       0.85      0.49      0.62      1700\n",
            "           6       0.79      0.36      0.49      4577\n",
            "           7       0.75      0.36      0.49      3712\n",
            "           8       0.92      0.72      0.81      4643\n",
            "           9       0.00      0.00      0.00        83\n",
            "          10       0.00      0.00      0.00        55\n",
            "          11       0.83      0.83      0.83      3717\n",
            "          12       0.00      0.00      0.00        43\n",
            "          13       0.83      0.44      0.58      1409\n",
            "          14       0.00      0.00      0.00        60\n",
            "          15       0.00      0.00      0.00        84\n",
            "          16       0.00      0.00      0.00        76\n",
            "          17       0.00      0.00      0.00        23\n",
            "\n",
            "   micro avg       0.98      0.96      0.97    395775\n",
            "   macro avg       0.53      0.41      0.46    395775\n",
            "weighted avg       0.97      0.96      0.96    395775\n",
            " samples avg       0.96      0.96      0.96    395775\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "y_pred_2e2 = m_2e2.predict(x_val)\n",
        "y_pred_2e2 = y_pred_2e2 > 0.5\n",
        "print(flat_classification_report(y_val, y_pred_2e2))  # , labels=tag_to_code))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## f) Free improvement\n",
        "\n",
        " Based on what has been experienced, we will try to improve the performance of the networks found.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "mean_sentencias = sum(largos_sentencias) / len(largos_sentencias)\n",
        "std = np.std(largos_sentencias)\n",
        "max_len = round(mean_sentencias + std.item())  # Promedio de largos de secuencias\n",
        "X = pad_sequences(\n",
        "    dff[\"lemma\"], maxlen=max_len, padding=\"pre\"\n",
        ")  # 0 al inicio por default\n",
        "# X\n",
        "Y = dff[\"tag\"].values\n",
        "Y = pad_sequences(Y, maxlen=max_len, padding=\"pre\")  # 0 al inicio por default\n",
        "y = np.asarray([to_categorical(i, num_classes=n_tags + 1) for i in Y])\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(X, y, test_size=0.25, random_state=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "207/207 [==============================] - 5s 17ms/step - loss: 1.3249 - acc: 0.7191 - val_loss: 0.2933 - val_acc: 0.9198\n",
            "Epoch 2/3\n",
            "207/207 [==============================] - 3s 14ms/step - loss: 0.2487 - acc: 0.9317 - val_loss: 0.1708 - val_acc: 0.9536\n",
            "Epoch 3/3\n",
            "207/207 [==============================] - 3s 14ms/step - loss: 0.1501 - acc: 0.9597 - val_loss: 0.1282 - val_acc: 0.9651\n"
          ]
        }
      ],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "from keras.layers import Embedding, Dense, GRU\n",
        "\n",
        "m_2f1 = Sequential()\n",
        "m_2f1.add(\n",
        "    Embedding(input_dim=n_lemmas + 1, output_dim=embedding_dim, input_length=max_len)\n",
        ")  ##,mask_zero=True))\n",
        "m_2f1.add(GRU(units=128, return_sequences=True))\n",
        "m_2f1.add(Dense(n_tags + 1, activation=\"softmax\"))\n",
        "m_2f1.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n",
        "history = m_2f1.fit(\n",
        "    x_tr, y_tr, validation_data=(x_val, y_val), epochs=3, batch_size=128\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    152888\n",
            "           1       0.97      0.99      0.98    206567\n",
            "           2       0.83      0.75      0.79      8734\n",
            "           3       0.96      0.86      0.91      3771\n",
            "           4       0.88      0.47      0.62      3633\n",
            "           5       0.88      0.41      0.56      1700\n",
            "           6       0.92      0.24      0.38      4577\n",
            "           7       0.89      0.16      0.27      3712\n",
            "           8       0.92      0.74      0.82      4643\n",
            "           9       0.00      0.00      0.00        83\n",
            "          10       0.00      0.00      0.00        55\n",
            "          11       0.84      0.72      0.78      3717\n",
            "          12       0.00      0.00      0.00        43\n",
            "          13       0.90      0.36      0.52      1409\n",
            "          14       0.00      0.00      0.00        60\n",
            "          15       0.00      0.00      0.00        84\n",
            "          16       0.00      0.00      0.00        76\n",
            "          17       0.00      0.00      0.00        23\n",
            "\n",
            "   micro avg       0.98      0.95      0.97    395775\n",
            "   macro avg       0.55      0.37      0.42    395775\n",
            "weighted avg       0.97      0.95      0.96    395775\n",
            " samples avg       0.95      0.95      0.95    395775\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn_crfsuite.metrics import flat_classification_report\n",
        "\n",
        "y_pred_2f1 = m_2f1.predict(x_val)\n",
        "y_pred_2f1 = y_pred_2f1 > 0.5\n",
        "print(flat_classification_report(y_val, y_pred_2f1))  # , labels=tag_to_code))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El tag predecido para el lemma on es: O\n",
            "El tag predecido para el lemma sunday es: B-tim\n",
            "El tag predecido para el lemma , es: O\n",
            "El tag predecido para el lemma the es: O\n",
            "El tag predecido para el lemma u.s. es: B-geo\n",
            "El tag predecido para el lemma militari es: O\n",
            "El tag predecido para el lemma said es: O\n",
            "El tag predecido para el lemma it es: O\n",
            "El tag predecido para el lemma had es: O\n",
            "El tag predecido para el lemma kill es: O\n",
            "El tag predecido para el lemma 15 es: O\n",
            "El tag predecido para el lemma to es: O\n",
            "El tag predecido para el lemma 20 es: O\n",
            "El tag predecido para el lemma suspect es: O\n",
            "El tag predecido para el lemma taleban es: B-org\n",
            "El tag predecido para el lemma fighter es: O\n",
            "El tag predecido para el lemma in es: O\n",
            "El tag predecido para el lemma an es: O\n",
            "El tag predecido para el lemma airstrik es: O\n",
            "El tag predecido para el lemma after es: O\n",
            "El tag predecido para el lemma coalit es: O\n",
            "El tag predecido para el lemma forc es: O\n",
            "El tag predecido para el lemma came es: O\n",
            "El tag predecido para el lemma under es: O\n",
            "El tag predecido para el lemma small es: O\n",
            "El tag predecido para el lemma arm es: O\n",
            "El tag predecido para el lemma and es: O\n",
            "El tag predecido para el lemma rocket es: O\n",
            "El tag predecido para el lemma fire es: O\n",
            "El tag predecido para el lemma . es: O\n"
          ]
        }
      ],
      "source": [
        "# 221\n",
        "example = 211\n",
        "pred_form = [\n",
        "    np.where(x == True)[0][0] - 1 if len(np.where(x == True)[0]) else 0\n",
        "    for x in y_pred_2f1[example]\n",
        "]\n",
        "for i in range(len(x_val[example])):\n",
        "    if x_val[example][i] == 0:\n",
        "        continue\n",
        "    print(\n",
        "        f\"El tag predecido para el lemma {lemmas[x_val[example][i]-1]} es: {tags[pred_form[i]]}\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## g) Let&#39;s write words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "##### I) Now we will look for another application to the recurrent networks, predict the next character. If we manage to train a network that is good at this task, we will be able to write text automatically, since we can, from a sentence, predict the next character, and then introduce the new sentence without the first character in the network again, and iterating thus to write automatically. Although recurrent networks are adequate for this task, we do not intend to train a Shakespeare, however it is interesting to investigate how plausible the generated text can be.\n",
        "\n",
        " For this, we will create our new dataset. For this task we will prefer to join all the sentences in a single large corpus and then create new semi-redundant sequences. This avoids us first of all the problem of having to do padding, since we will create all the entries the same, but it also allows us to take better advantage of the dataset, in a certain way increasing the number of data. The target in this case will only be the next character corresponding to each sequence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Skipping line 281837: Expected 25 fields in line 281837, saw 34\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "# Cargamos el dataframe\n",
        "df_w = pd.read_csv(os.path.join(\"ner.csv\"), engine=\"python\", error_bad_lines=False)\n",
        "# Elimina todos los registros con valores nan y toma la columna word\n",
        "df_w = df_w.dropna()[[\"word\"]]\n",
        "\n",
        "corpus = \" \".join(\n",
        "    list(df_w.word.values)\n",
        ").lower()  # unimos todas las \"palabras\" en minuscula en un gran string\n",
        "sentence_length = 40  # definimos el largo de la secuencia\n",
        "steps = 5  # definimos el salto entre sentencia y sentencia, como es menor que 40 iremos repitiendo caracteres\n",
        "\n",
        "# Lista para guardas las sentencias\n",
        "sentences = []\n",
        "# Lista para guardar los caracteres siguientes\n",
        "next_char = []\n",
        "# Itera sobre el corpus que contiene todas las palabras en minúscula con pasos de a 5 palabras\n",
        "for i in range(0, len(corpus) - sentence_length - 1, steps):\n",
        "    # Arma las sentencias con el largo de la sentencia definido previamente, se destaca que, al ser el largo de las secuencias mayor que el step, cada secuencia\n",
        "    # contendrá palabras repetidas de sentencias anteriores\n",
        "    sentences.append(corpus[i : sentence_length + i])\n",
        "    # Se añade el siguiente caracter de la secuencia recién añadida, es decir, el valor a predecir\n",
        "    next_char.append(corpus[sentence_length + i])\n",
        "\n",
        "# Codificamos cada caracter presente en el corpus\n",
        "chars_to_code = {char: code for code, char in enumerate(set(corpus))}\n",
        "# Decodificamos cada codigo al caracter correspondiente\n",
        "code_to_chars = {code: char for char, code in chars_to_code.items()}\n",
        "# Codificamos cada sentencia de entrenamiento\n",
        "x = np.array([[chars_to_code[char] for char in sentence] for sentence in sentences])\n",
        "# Codificamos cada caracter de output para una sentencia en específico\n",
        "y = np.array([chars_to_code[char] for char in next_char])\n",
        "# Pasamosa a one hot vector las predicciones\n",
        "y = to_categorical(y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "##### II) We will train a network with this data using GRU.\n",
        "\n",
        " If we were to train the network with a different dataset, the texts generated for the same initial sequence would be different. This is because in learning they would learn\n",
        "\n",
        " different relationships, thus obtaining models that behave differently when faced with the same output. For example, if we train a model\n",
        "\n",
        " With a dataset of movie opinions, it is expected that the generated texts are based mainly on the names of actresses, actors, movies, characters, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.layers import Embedding\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import GRU\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "\n",
        "\n",
        "def predict_char(model, sentence):\n",
        "    x = [chars_to_code[char] for char in sentence]\n",
        "    x = pad_sequences([x], maxlen=sentence_length, padding=\"pre\", value=0)\n",
        "    probas = model.predict(x)[0]\n",
        "    next_index = np.random.choice(len(chars_to_code), p=probas)\n",
        "    return code_to_chars[next_index]\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, logs):\n",
        "    print(f\"\\n Generating random text for epoch: {epoch}\")\n",
        "    start_index = random.randint(0, x.shape[0] - 1)\n",
        "    sentence = \"\".join([code_to_chars[code] for code in x[start_index]])\n",
        "    print(\"\\n Generating with seed: \" + sentence)\n",
        "    sys.stdout.write(sentence)\n",
        "    for i in range(400):\n",
        "        next_char = predict_char(character, sentence)\n",
        "        sentence = sentence[1:] + next_char  # for next character\n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "    return\n",
        "\n",
        "\n",
        "print_text_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2371/2371 [==============================] - 30s 12ms/step - loss: 1.8508 - acc: 0.4604\n",
            "\n",
            " Generating random text for epoch: 0\n",
            "\n",
            " Generating with seed: pine army camp , where u.s. soldiers and\n",
            "pine army camp , where u.s. soldiers and fcoccement obchunters forced officiancens stand dri . decupted due willed days will near days on during friday porsman jodes for gaymaks say decoup , and forly misio has and killed stands wednsund fives elutivened trand to overivers three on tued injasas killed in legged tile runs officials mud days in the cancem for the terrists for nissing for dismonth in onew af quccad say that culoods jontiveModel: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 40, 100)           6000      \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 128)               88320     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 60)                7740      \n",
            "=================================================================\n",
            "Total params: 102,060\n",
            "Trainable params: 102,060\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "character = Sequential()\n",
        "embedding_dim = 100\n",
        "character.add(\n",
        "    Embedding(input_dim=y.shape[1], output_dim=embedding_dim, input_length=x.shape[1])\n",
        ")\n",
        "character.add(GRU(128, return_sequences=False))\n",
        "character.add(Dense(y.shape[1], activation=\"softmax\"))\n",
        "optimizer = RMSprop(lr=0.01)\n",
        "character.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"acc\"])\n",
        "character.fit(x, y, epochs=1, batch_size=512, callbacks=[print_text_callback])\n",
        "character.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the flowers are mites for the dispressments in the gensons the outside fund days of the glans thats fromer bringin \n"
          ]
        }
      ],
      "source": [
        "predicted_char = \"the flowers are\"\n",
        "for i in range(100):\n",
        "    predicted_char = predicted_char + predict_char(character, predicted_char)\n",
        "print(predicted_char)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "2371/2371 [==============================] - 37s 15ms/step - loss: 1.8973 - acc: 0.4475\n",
            "Epoch 2/2\n",
            "2371/2371 [==============================] - 34s 15ms/step - loss: 1.6241 - acc: 0.5196\n",
            "Model: \"sequential_64\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_64 (Embedding)     (None, 40, 100)           6000      \n",
            "_________________________________________________________________\n",
            "gru_130 (GRU)                (None, 40, 32)            12864     \n",
            "_________________________________________________________________\n",
            "gru_131 (GRU)                (None, 128)               62208     \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (None, 60)                7740      \n",
            "=================================================================\n",
            "Total params: 88,812\n",
            "Trainable params: 88,812\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "character_2 = Sequential()\n",
        "embedding_dim = 100\n",
        "character_2.add(\n",
        "    Embedding(input_dim=y.shape[1], output_dim=embedding_dim, input_length=x.shape[1])\n",
        ")\n",
        "character_2.add(GRU(32, return_sequences=True))\n",
        "character_2.add(GRU(128, return_sequences=False))\n",
        "character_2.add(Dense(y.shape[1], activation=\"softmax\"))\n",
        "optimizer = RMSprop(lr=0.01)\n",
        "character_2.compile(\n",
        "    loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"acc\"]\n",
        ")\n",
        "character_2.fit(x, y, epochs=2, batch_size=512)\n",
        "character_2.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "last year in francely citia . ghbern ivan cascaced of camicals in a peonzecparitia 's hospriciated instkreeptant groazw\n"
          ]
        }
      ],
      "source": [
        "predicted_char = \"last year in france\"\n",
        "for i in range(100):\n",
        "    predicted_char = predicted_char + predict_char(character_2, predicted_char)\n",
        "print(predicted_char)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
